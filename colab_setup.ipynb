{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ-VAE Colab Setup & Testing\n",
    "\n",
    "This notebook sets up the RQ-VAE project in Google Colab for iterative testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "!git clone https://github.com/YOUR_USERNAME/rq_vae.git\n",
    "%cd rq_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch>=2.0.0\n",
    "!pip install -q transformers datasets wandb einops omegaconf tqdm accelerate\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "print(\"✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import and Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.model import TextEncoder, TextDecoder, SwiGLU, SwiGLUTransformerLayer\n",
    "\n",
    "print(f\"✓ Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating encoder...\")\n",
    "\n",
    "encoder = TextEncoder(\n",
    "    model_name=\"Qwen/Qwen3-0.6B\",\n",
    "    latent_dim=512,\n",
    "    compression_factor=4,\n",
    "    freeze_backbone=True,\n",
    "    num_latent_layers=2,\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "print(f\"✓ Encoder created on {device}\")\n",
    "print(f\"  Model: {encoder.model_name}\")\n",
    "print(f\"  Hidden size: {encoder.hidden_size}\")\n",
    "print(f\"  Latent dim: {encoder.latent_dim}\")\n",
    "print(f\"  Compression: {encoder.compression_factor}x\")\n",
    "print(f\"  Latent layers: {encoder.num_latent_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoder forward pass\n",
    "print(\"Testing encoder forward pass...\")\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "input_ids = torch.randint(0, 1000, (batch_size, seq_len)).to(device)\n",
    "attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = encoder(input_ids, attention_mask)\n",
    "\n",
    "print(f\"✓ Encoder forward pass successful!\")\n",
    "print(f\"  Input shape:  {input_ids.shape}\")\n",
    "print(f\"  Output shape: {latents.shape}\")\n",
    "print(f\"  Compression:  {seq_len} -> {latents.shape[1]} tokens ({seq_len // latents.shape[1]}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating decoder...\")\n",
    "\n",
    "decoder = TextDecoder(\n",
    "    model_name=\"Qwen/Qwen3-0.6B\",\n",
    "    latent_dim=512,\n",
    "    compression_factor=4,\n",
    "    freeze_backbone=True,\n",
    "    num_latent_layers=2,\n",
    ")\n",
    "\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "print(f\"✓ Decoder created on {device}\")\n",
    "print(f\"  Model: {decoder.model_name}\")\n",
    "print(f\"  Hidden size: {decoder.hidden_size}\")\n",
    "print(f\"  Latent dim: {decoder.latent_dim}\")\n",
    "print(f\"  Vocab size: {decoder.vocab_size}\")\n",
    "print(f\"  Compression: {decoder.compression_factor}x\")\n",
    "print(f\"  Latent layers: {decoder.num_latent_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decoder forward pass\n",
    "print(\"Testing decoder forward pass...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = decoder(latents, target_len=seq_len)\n",
    "\n",
    "print(f\"✓ Decoder forward pass successful!\")\n",
    "print(f\"  Input shape:  {latents.shape}\")\n",
    "print(f\"  Output shape: {logits.shape}\")\n",
    "print(f\"  Expansion:    {latents.shape[1]} -> {logits.shape[1]} tokens ({logits.shape[1] // latents.shape[1]}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Full Pipeline (Encode -> Decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing full encode -> decode pipeline...\")\n",
    "\n",
    "# Create fresh input\n",
    "batch_size = 2\n",
    "seq_len = 256\n",
    "input_ids = torch.randint(0, decoder.vocab_size, (batch_size, seq_len)).to(device)\n",
    "attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode\n",
    "    latents = encoder(input_ids, attention_mask)\n",
    "    \n",
    "    # Decode\n",
    "    reconstructed_logits = decoder(latents, target_len=seq_len)\n",
    "    \n",
    "    # Get predicted tokens\n",
    "    predicted_tokens = reconstructed_logits.argmax(dim=-1)\n",
    "\n",
    "print(f\"✓ Full pipeline successful!\")\n",
    "print(f\"  Input tokens:    {input_ids.shape}\")\n",
    "print(f\"  Compressed:      {latents.shape}\")\n",
    "print(f\"  Reconstructed:   {reconstructed_logits.shape}\")\n",
    "print(f\"  Predicted:       {predicted_tokens.shape}\")\n",
    "\n",
    "# Calculate reconstruction accuracy (just for fun, won't be good without training)\n",
    "accuracy = (predicted_tokens == input_ids).float().mean().item()\n",
    "print(f\"  Token accuracy:  {accuracy:.2%} (untrained, just a sanity check)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Max allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test SwiGLU Layers (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing SwiGLU components...\")\n",
    "\n",
    "# Test SwiGLU\n",
    "swiglu = SwiGLU(dim=512).to(device)\n",
    "x = torch.randn(2, 32, 512).to(device)\n",
    "with torch.no_grad():\n",
    "    out = swiglu(x)\n",
    "print(f\"✓ SwiGLU: {x.shape} -> {out.shape}\")\n",
    "\n",
    "# Test SwiGLUTransformerLayer\n",
    "layer = SwiGLUTransformerLayer(d_model=512, nhead=8).to(device)\n",
    "with torch.no_grad():\n",
    "    out = layer(x)\n",
    "print(f\"✓ SwiGLUTransformerLayer: {x.shape} -> {out.shape}\")\n",
    "\n",
    "print(\"\\n✓ All components working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Training Test (Optional)\n",
    "\n",
    "Test that gradients flow properly (without actual training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing gradient flow...\")\n",
    "\n",
    "# Unfreeze models for gradient test\n",
    "encoder.unfreeze_backbone()\n",
    "decoder.unfreeze_backbone()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Forward pass with gradients\n",
    "input_ids = torch.randint(0, decoder.vocab_size, (2, 64)).to(device)\n",
    "attention_mask = torch.ones(2, 64, dtype=torch.long).to(device)\n",
    "\n",
    "latents = encoder(input_ids, attention_mask)\n",
    "logits = decoder(latents, target_len=64)\n",
    "\n",
    "# Dummy loss\n",
    "loss = torch.nn.functional.cross_entropy(\n",
    "    logits.reshape(-1, decoder.vocab_size),\n",
    "    input_ids.reshape(-1)\n",
    ")\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"✓ Gradients flow correctly!\")\n",
    "print(f\"  Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Re-freeze for future tests\n",
    "encoder.freeze_backbone()\n",
    "decoder.freeze_backbone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ All Tests Complete!\n",
    "\n",
    "Your RQ-VAE models are working correctly in Colab. You can now:\n",
    "- Train the models\n",
    "- Test with real data\n",
    "- Experiment with hyperparameters"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
